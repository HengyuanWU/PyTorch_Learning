{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Transformer 实验与调参手册 (AG NEWS)\n",
        "\n",
        "## 目录\n",
        "- [环境准备](#环境准备)\n",
        "- [单次 Baseline 训练](#单次-Baseline-训练)\n",
        "- [超参数网格搜索](#超参数网格搜索)\n",
        "- [结果汇总表](#结果汇总表)\n",
        "- [曲线可视化](#曲线可视化)\n",
        "- [拆零件小实验](#拆零件小实验)\n",
        "- [Attention 可视化](#Attention-可视化)\n",
        "- [结论总结](#结论总结)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 环境准备\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, torch, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
        "import numpy as np\n",
        "import subprocess\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "sys.path.append(\"..\")      # 项目根\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "print(torch.__version__, torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 单次 Baseline 训练\n",
        "\n",
        "首先运行一次基础训练，确保代码无错误且能成功训练。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 运行基础训练\n",
        "!python ../train/train_transformer.py --epochs 5 --batch-size 64 --lr 1e-3 --embed-dim 128\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 超参数网格搜索\n",
        "\n",
        "定义一组系统化的超参数组合，进行网格搜索实验。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 定义超参数网格\n",
        "grid = [\n",
        "    {\"embed_dim\": d, \"num_heads\": h, \"lr\": lr}\n",
        "    for d, h in [(64, 2), (128, 4), (256, 8)]\n",
        "    for lr in [5e-4, 1e-3]\n",
        "]\n",
        "results = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 函数用于解析输出中的验证准确率\n",
        "def parse_output(output_str):\n",
        "    lines = output_str.split('\\n')\n",
        "    val_acc = None\n",
        "    for line in lines:\n",
        "        if \"Val loss\" in line:\n",
        "            parts = line.split(',')\n",
        "            if len(parts) > 1:\n",
        "                acc_part = parts[1].strip()\n",
        "                val_acc = float(acc_part.replace('Acc: ', '').replace('%', ''))\n",
        "    return val_acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 循环执行每个超参数组合的实验\n",
        "for i, params in enumerate(grid):\n",
        "    print(f\"\\n实验 {i+1}/{len(grid)}: {params}\")\n",
        "    \n",
        "    # 构建命令\n",
        "    cmd = [\n",
        "        \"python\", \"../train/train_transformer.py\",\n",
        "        \"--epochs\", \"3\",\n",
        "        \"--batch-size\", \"64\",\n",
        "        \"--embed-dim\", str(params['embed_dim']),\n",
        "        \"--num-heads\", str(params['num_heads']),\n",
        "        \"--lr\", str(params['lr'])\n",
        "    ]\n",
        "    \n",
        "    # 执行命令并捕获输出\n",
        "    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "    stdout, stderr = process.communicate()\n",
        "    \n",
        "    # 解析验证准确率\n",
        "    val_acc = parse_output(stdout)\n",
        "    \n",
        "    # 将结果添加到列表\n",
        "    result = params.copy()\n",
        "    result['val_acc'] = val_acc\n",
        "    results.append(result)\n",
        "    \n",
        "    print(f\"验证准确率: {val_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 结果汇总表\n",
        "\n",
        "将网格搜索结果整理成 DataFrame 并排序。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 转换为 DataFrame\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# 按验证准确率降序排序\n",
        "df_sorted = df.sort_values(by='val_acc', ascending=False)\n",
        "display(df_sorted)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 曲线可视化\n",
        "\n",
        "绘制超参数对性能影响的曲线图。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# 1. Embedding维度对准确率影响\n",
        "sns.lineplot(data=df, x=\"embed_dim\", y=\"val_acc\", hue=\"num_heads\", style=\"lr\")\n",
        "plt.title(\"Embedding 维度 / Head 数 / LR 对准确率影响\")\n",
        "plt.xlabel(\"Embedding 维度\")\n",
        "plt.ylabel(\"验证准确率 (%)\")\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.savefig('../outputs/transformer/param_impact.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. 学习率对不同模型规模的影响\n",
        "g = sns.catplot(\n",
        "    data=df, x=\"lr\", y=\"val_acc\", \n",
        "    col=\"embed_dim\", hue=\"num_heads\",\n",
        "    kind=\"bar\", height=5, aspect=0.8\n",
        ")\n",
        "g.set_axis_labels(\"学习率\", \"验证准确率 (%)\")\n",
        "g.set_titles(\"Embedding维度: {col_name}\")\n",
        "plt.savefig('../outputs/transformer/lr_impact.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 拆零件小实验\n",
        "\n",
        "进行6种结构变体实验，观察各组件对模型性能的影响。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 定义消融实验列表\n",
        "ablations = [\n",
        "    \"no_pe\",        # 关闭位置编码\n",
        "    \"single_head\",  # 只用单头注意力\n",
        "    \"no_ffn\",       # 移除前馈网络\n",
        "    \"freeze_emb\",   # 冻结embedding\n",
        "    \"no_dropout\",   # 关闭dropout\n",
        "    \"clip_grad\"     # 梯度裁剪\n",
        "]\n",
        "\n",
        "# 获取最佳配置作为基准\n",
        "best_config = df_sorted.iloc[0].to_dict()\n",
        "embed_dim = int(best_config['embed_dim'])\n",
        "num_heads = int(best_config['num_heads'])\n",
        "lr = best_config['lr']\n",
        "\n",
        "ablation_results = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 先运行一次基础模型（无消融）作为对照\n",
        "print(\"运行基准模型（无消融）...\")\n",
        "cmd = [\n",
        "    \"python\", \"../train/train_transformer.py\",\n",
        "    \"--epochs\", \"3\",\n",
        "    \"--batch-size\", \"64\",\n",
        "    \"--embed-dim\", str(embed_dim),\n",
        "    \"--num-heads\", str(num_heads),\n",
        "    \"--lr\", str(lr)\n",
        "]\n",
        "process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "stdout, stderr = process.communicate()\n",
        "val_acc = parse_output(stdout)\n",
        "ablation_results.append({\n",
        "    \"ablation\": \"baseline\",\n",
        "    \"description\": \"基准模型（无消融）\",\n",
        "    \"val_acc\": val_acc\n",
        "})\n",
        "print(f\"基准模型验证准确率: {val_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 运行各种消融实验\n",
        "descriptions = {\n",
        "    \"no_pe\": \"关闭位置编码\",\n",
        "    \"single_head\": \"只用单头注意力\",\n",
        "    \"no_ffn\": \"移除前馈网络\",\n",
        "    \"freeze_emb\": \"冻结embedding\",\n",
        "    \"no_dropout\": \"关闭dropout\",\n",
        "    \"clip_grad\": \"梯度裁剪\"\n",
        "}\n",
        "\n",
        "for ablation in ablations:\n",
        "    print(f\"\\n运行消融实验: {ablation} - {descriptions[ablation]}\")\n",
        "    cmd = [\n",
        "        \"python\", \"../train/train_transformer.py\",\n",
        "        \"--epochs\", \"3\",\n",
        "        \"--batch-size\", \"64\",\n",
        "        \"--embed-dim\", str(embed_dim),\n",
        "        \"--num-heads\", str(num_heads),\n",
        "        \"--lr\", str(lr),\n",
        "        \"--ablation\", ablation\n",
        "    ]\n",
        "    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "    stdout, stderr = process.communicate()\n",
        "    val_acc = parse_output(stdout)\n",
        "    ablation_results.append({\n",
        "        \"ablation\": ablation,\n",
        "        \"description\": descriptions[ablation],\n",
        "        \"val_acc\": val_acc\n",
        "    })\n",
        "    print(f\"验证准确率: {val_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 将消融实验结果整理为DataFrame\n",
        "ablation_df = pd.DataFrame(ablation_results)\n",
        "\n",
        "# 计算相对于基准的性能变化百分比\n",
        "# 获取基准值\n",
        "baseline_rows = ablation_df[ablation_df['ablation'] == 'baseline']\n",
        "if len(baseline_rows) > 0:\n",
        "    baseline_acc = list(baseline_rows['val_acc'])[0]\n",
        "else:\n",
        "    baseline_acc = 0\n",
        "ablation_df['rel_change'] = (ablation_df['val_acc'] - baseline_acc) / baseline_acc * 100\n",
        "\n",
        "# 排序并显示结果\n",
        "ablation_sorted = ablation_df.sort_values(by='val_acc', ascending=False)\n",
        "display(ablation_sorted)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 可视化消融实验结果\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# 不包含baseline的数据\n",
        "plot_df = ablation_df[ablation_df['ablation'] != 'baseline'].copy()\n",
        "\n",
        "# 根据相对变化排序 (这将正常工作，尽管IDE可能显示警告)\n",
        "# sort_values是pandas的标准方法，运行时不会出错\n",
        "plot_df = plot_df.sort_values('rel_change')\n",
        "\n",
        "# 创建柱状图\n",
        "bars = plt.bar(plot_df['description'], plot_df['rel_change'])\n",
        "\n",
        "# 为负值设置不同颜色\n",
        "for i, v in enumerate(plot_df['rel_change']):\n",
        "    if v < 0:\n",
        "        bars[i].set_alpha(0.7)\n",
        "\n",
        "plt.axhline(y=0, color='r', linestyle='-', alpha=0.3)\n",
        "plt.title(f\"各种结构变体对模型性能的影响 (相对于基准 {baseline_acc:.2f}%)\", fontsize=14)\n",
        "plt.xlabel(\"结构变体\")\n",
        "plt.ylabel(\"相对准确率变化 (%)\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.savefig('../outputs/transformer/ablation_results.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Attention 可视化\n",
        "\n",
        "加载最高分模型，选择一条验证集样本，可视化第1层第1个头的注意力热力图。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "import torch\n",
        "from models.transformer import Transformer\n",
        "from utils.text_dataloader import get_ag_news_dataloader\n",
        "from pathlib import Path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 加载最佳模型\n",
        "# 获取验证数据加载器\n",
        "data_dir = Path(\"../data\")\n",
        "val_loader, vocab, vocab_size, num_classes = get_ag_news_dataloader(\n",
        "    batch_size=1,  # 单条样本\n",
        "    max_len=128,   # 限制长度以便可视化\n",
        "    train=False,\n",
        "    root=str(data_dir)\n",
        ")\n",
        "\n",
        "# 根据最佳超参数创建模型\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Transformer(\n",
        "    vocab_size=vocab_size,\n",
        "    embed_dim=embed_dim,\n",
        "    num_heads=num_heads,\n",
        "    hidden_dim=embed_dim*4,  # 标准Transformer通常使用4倍embed_dim作为FFN隐藏层\n",
        "    num_layers=3,\n",
        "    num_classes=num_classes\n",
        ")\n",
        "\n",
        "# 加载最佳检查点\n",
        "best_model_path = Path(\"../outputs/transformer/transformer_best.pth\")\n",
        "if best_model_path.exists():\n",
        "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "    print(f\"已加载最佳模型: {best_model_path}\")\n",
        "else:\n",
        "    print(f\"找不到最佳模型文件，使用未训练模型\")\n",
        "\n",
        "model = model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 提取单个Transformer层注意力权重的钩子函数\n",
        "class AttentionHook:\n",
        "    def __init__(self):\n",
        "        self.attn_weights = None\n",
        "        \n",
        "    def __call__(self, module, module_input, module_output):\n",
        "        # PyTorch TransformerEncoder层的attention weights在运行时不保存\n",
        "        # 我们需要修改模型结构以访问它们\n",
        "        # 这里仅为示例，实际使用时需调整\n",
        "        self.attn_weights = None  # 稍后会在前向传播中手动提取\n",
        "\n",
        "\n",
        "# 修改Transformer模型来返回注意力权重\n",
        "def forward_with_attention(model, x, src_key_padding_mask=None):\n",
        "    \"\"\"修改的前向传播，返回注意力权重\"\"\"\n",
        "    # 词嵌入 [batch_size, seq_len] -> [batch_size, seq_len, embed_dim]\n",
        "    embedded = model.embedding(x)\n",
        "    \n",
        "    # 添加位置编码\n",
        "    embedded = model.pos_encoder(embedded)\n",
        "    \n",
        "    # 我们需要访问TransformerEncoder的第一层的第一个注意力头\n",
        "    # 由于PyTorch没有直接暴露这个API，我们需要手动添加钩子或修改模型\n",
        "    # 这里是一个近似方法，实际实现可能需要更深入地修改模型\n",
        "    \n",
        "    # 使用第一个编码器层的self-attention模块\n",
        "    first_layer = model.transformer_encoder.layers[0]\n",
        "    \n",
        "    # 1. 应用自注意力，并保存注意力权重\n",
        "    # 这部分逻辑通常在TransformerEncoderLayer的内部\n",
        "    q = first_layer.self_attn.q_proj(embedded)\n",
        "    k = first_layer.self_attn.k_proj(embedded)\n",
        "    v = first_layer.self_attn.v_proj(embedded)\n",
        "    \n",
        "    # 重塑为多头形式\n",
        "    batch_size, seq_len = embedded.shape[0], embedded.shape[1]\n",
        "    head_dim = model.transformer_encoder.layers[0].self_attn.head_dim\n",
        "    num_heads = model.transformer_encoder.layers[0].self_attn.num_heads\n",
        "    \n",
        "    q = q.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)\n",
        "    k = k.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)\n",
        "    v = v.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)\n",
        "    \n",
        "    # 计算注意力分数\n",
        "    scores = torch.matmul(q, k.transpose(-2, -1)) / (head_dim ** 0.5)\n",
        "    \n",
        "    # 应用掩码（如果有）\n",
        "    if src_key_padding_mask is not None:\n",
        "        # 扩展掩码以匹配分数的形状\n",
        "        mask = src_key_padding_mask.unsqueeze(1).unsqueeze(2)\n",
        "        scores = scores.masked_fill(mask, float(\"-inf\"))\n",
        "    \n",
        "    # 应用softmax得到注意力权重\n",
        "    attn_weights = torch.nn.functional.softmax(scores, dim=-1)\n",
        "    \n",
        "    # 提取第一个头的注意力权重\n",
        "    first_head_attn = attn_weights[0, 0].cpu().detach()\n",
        "    \n",
        "    # 继续常规前向传播\n",
        "    # 由于我们不需要完整的前向传播结果，这里简化了步骤\n",
        "    \n",
        "    return first_head_attn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 选择一条验证样本\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # 获取一个批次\n",
        "    for tokens, labels, masks in val_loader:\n",
        "        tokens, labels, masks = tokens.to(device), labels.to(device), masks.to(device)\n",
        "        \n",
        "        # 使用修改后的前向传播函数获取注意力权重\n",
        "        attention_weights = forward_with_attention(model, tokens, src_key_padding_mask=masks)\n",
        "        \n",
        "        # 获取真实标记（而不是填充标记）\n",
        "        valid_seq_len = (~masks[0]).sum().item()\n",
        "        \n",
        "        # 可视化注意力热力图\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        \n",
        "        # 只显示有效的标记（不显示填充标记）\n",
        "        valid_attn = attention_weights[:valid_seq_len, :valid_seq_len]\n",
        "        \n",
        "        # 创建热力图\n",
        "        sns.heatmap(valid_attn, cmap=\"viridis\")\n",
        "        plt.title(f\"第1层第1个头的Self-Attention热力图 (类别: {labels.item()+1})\")\n",
        "        plt.xlabel(\"Token位置\")\n",
        "        plt.ylabel(\"Token位置\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('../outputs/transformer/attention_heatmap.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        \n",
        "        # 一条样本就够了\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 结论总结\n",
        "\n",
        "1. **超参数实验结论**\n",
        "   - 最佳超参数组合及其性能表现\n",
        "   - embedding维度与head数量的关系\n",
        "   - 学习率对不同模型规模的影响\n",
        "\n",
        "2. **结构变体实验结论**\n",
        "   - 各组件对模型性能的影响排序\n",
        "   - 哪些组件是必不可少的，哪些影响较小\n",
        "\n",
        "3. **注意力可视化分析**\n",
        "   - 模型关注了哪些位置的token\n",
        "   - 是否存在明显的注意力模式\n",
        "\n",
        "4. **TODO 与改进方向**\n",
        "   - 尝试更多超参数组合\n",
        "   - 增加序列长度或批量大小\n",
        "   - 尝试不同的位置编码方案\n",
        "   - 探索预训练与微调策略\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
